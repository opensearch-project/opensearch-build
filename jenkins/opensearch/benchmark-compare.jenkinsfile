/*
 * Copyright OpenSearch Contributors
 * SPDX-License-Identifier: Apache-2.0
 *
 * The OpenSearch Contributors require contributions made to
 * this file be licensed under the Apache-2.0 license or a
 * compatible open source license.
 */

lib = library(identifier: 'jenkins@6.8.2', retriever: modernSCM([

    $class: 'GitSCMSource',
    remote: 'https://github.com/opensearch-project/opensearch-build-libraries.git',
]))

pipeline {
    agent { label 'Jenkins-Agent-AL2023-X64-M52xlarge-Benchmark-Test' }
    options {
        timeout(time: 1, unit: 'HOURS')
        buildDiscarder(logRotator(daysToKeepStr: '30'))
        throttleJobProperty(
             categories: [],
             limitOneJobWithMatchingParams: false,
             maxConcurrentPerNode: 0,
             maxConcurrentTotal: 20,
             paramsToUseForLimit: '',
             throttleEnabled: true,
             throttleOption: 'project',
        )
    }
    environment {
        JOB_NAME = 'benchmark-compare'
    }
    parameters {
        string(
            name: 'BASELINE_TEST_EXECUTION_ID',
            description: 'opensearch-benchmark test-execution-id for baseline run, sample format: `61a34b4e-4ae4-40b7-9d58-6570b9166f6a`',
            trim: true
        )
        string(
            name: 'CONTENDER_TEST_EXECUTION_ID',
            description: 'opensearch-benchmark test-execution-id of candidate for comparison, mainly from pull requests.',
            trim: true
        )
        string(
            name: 'PULL_REQUEST_NUMBER',
            trim: true
        )
        string(
            name: 'REPOSITORY',
            defaultValue: 'opensearch-project/OpenSearch'
        )
    }
    stages {
        stage('validate-and-set-parameters') {
            steps {
                script {
                    if (BASELINE_TEST_EXECUTION_ID == '' || CONTENDER_TEST_EXECUTION_ID == '') {
                        currentBuild.result = 'ABORTED'
                        error('Benchmark test failed to start. Please provide BASELINE_TEST_EXECUTION_ID and CONTENDER_TEST_EXECUTION_ID to run the job.')
                    }
                }
            }
        }
        stage('benchmark-pull-request') {
            steps {
                script {
                    runBenchmarkTestScript(
                        command: 'compare',
                        baseline: "${BASELINE_TEST_EXECUTION_ID}",
                        contender: "${CONTENDER_TEST_EXECUTION_ID}",
                        suffix: "${BUILD_NUMBER}"
                    )
                }
            }
            post {
                success {
                    sh """
                        echo '<details><summary>Benchmark Baseline Comparison Results</summary>' > temp_result_${BUILD_NUMBER}.md
                        echo '' >> temp_result_${BUILD_NUMBER}.md
                        echo '#### Benchmark Results for Job: ${BUILD_URL}' >> temp_result_${BUILD_NUMBER}.md
                        cat final_result_${BUILD_NUMBER}.md >> temp_result_${BUILD_NUMBER}.md
                        echo '</details>' >> temp_result_${BUILD_NUMBER}.md
                        mv temp_result_${BUILD_NUMBER}.md final_result_${BUILD_NUMBER}.md
                    """
                    script {
                        if ("${PULL_REQUEST_NUMBER}" != '' && "${REPOSITORY}" != '') {
                            withCredentials([usernamePassword(credentialsId: 'jenkins-github-bot-token', passwordVariable: 'GITHUB_TOKEN', usernameVariable: 'GITHUB_USER')]) {
                                def pull_request = Integer.parseInt("${PULL_REQUEST_NUMBER}")
                                sh ("gh pr comment ${pull_request} --repo ${REPOSITORY} --body-file final_result_${BUILD_NUMBER}.md")
                            }
                        }
                    }
                    postCleanup()
                }
                failure {
                    postCleanup()
                }
            }
        }
    }
}
